#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GraphRAG å›¾ä¹¦å¯è§†åŒ–å’Œæ™ºèƒ½æŸ¥è¯¢ç³»ç»Ÿ
ç”¨äºå¯è§†åŒ–æç¤ºè¯å†™ä½œå›¾ä¹¦çš„çŸ¥è¯†å›¾è°±å¹¶ç”Ÿæˆæ·±åº¦æŒ–æ˜é—®é¢˜
"""

import os
import json
import subprocess
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import xml.etree.ElementTree as ET
from collections import Counter, defaultdict
import random
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class GraphRAGVisualizer:
    def __init__(self, artifacts_path):
        """
        åˆå§‹åŒ–GraphRAGå¯è§†åŒ–å™¨
        
        Args:
            artifacts_path (str): GraphRAGè¾“å‡ºçš„artifactsç›®å½•è·¯å¾„
        """
        self.artifacts_path = Path(artifacts_path)
        self.ragtest_path = Path("C:/Users/13694/ragtest")
        self.graph_data = {}
        self.entities = []
        self.relationships = []
        
    def load_graph_data(self):
        """åŠ è½½GraphRAGç”Ÿæˆçš„å›¾æ•°æ®"""
        try:
            # åŠ è½½GraphMLæ–‡ä»¶
            graphml_file = self.artifacts_path / "summarized_graph.graphml"
            if graphml_file.exists():
                self.graph_data['graph'] = nx.read_graphml(str(graphml_file))
                print(f"âœ… æˆåŠŸåŠ è½½å›¾æ•°æ®ï¼ŒåŒ…å« {len(self.graph_data['graph'].nodes)} ä¸ªèŠ‚ç‚¹å’Œ {len(self.graph_data['graph'].edges)} æ¡è¾¹")
            
            # åŠ è½½å®ä½“æ•°æ®
            entities_file = self.artifacts_path / "raw_extracted_entities.json"
            if entities_file.exists():
                with open(entities_file, 'r', encoding='utf-8') as f:
                    self.entities = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.entities)} ä¸ªå®ä½“")
            
            # åŠ è½½é¡¶å±‚èŠ‚ç‚¹
            top_nodes_file = self.artifacts_path / "top_level_nodes.json"
            if top_nodes_file.exists():
                with open(top_nodes_file, 'r', encoding='utf-8') as f:
                    self.graph_data['top_nodes'] = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½é¡¶å±‚èŠ‚ç‚¹æ•°æ®")
                
            # åŠ è½½ç»Ÿè®¡ä¿¡æ¯
            stats_file = self.artifacts_path / "stats.json"
            if stats_file.exists():
                with open(stats_file, 'r', encoding='utf-8') as f:
                    self.graph_data['stats'] = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½ç»Ÿè®¡ä¿¡æ¯")
                
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def analyze_book_structure(self):
        """åˆ†æå›¾ä¹¦ç»“æ„å’Œä¸»é¢˜"""
        print("\nğŸ“Š åˆ†æå›¾ä¹¦ç»“æ„...")
        
        # åˆ†æå®ä½“ç±»å‹åˆ†å¸ƒ
        entity_types = Counter()
        entity_descriptions = []
        
        for entity in self.entities:
            if 'type' in entity:
                entity_types[entity['type']] += 1
            if 'description' in entity:
                entity_descriptions.append(entity['description'])
        
        print(f"ğŸ“ˆ å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in entity_types.most_common(10):
            print(f"  - {entity_type}: {count}")
        
        # åˆ†æå›¾ç»“æ„
        if 'graph' in self.graph_data:
            G = self.graph_data['graph']
            print(f"\nğŸ”— å›¾ç»“æ„åˆ†æ:")
            print(f"  - èŠ‚ç‚¹æ•°é‡: {G.number_of_nodes()}")
            print(f"  - è¾¹æ•°é‡: {G.number_of_edges()}")
            print(f"  - å¯†åº¦: {nx.density(G):.4f}")
            print(f"  - è¿é€šç»„ä»¶æ•°: {nx.number_connected_components(G.to_undirected())}")
        
        return entity_types, entity_descriptions
    
    def visualize_knowledge_graph(self):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        print("\nğŸ¨ ç”ŸæˆçŸ¥è¯†å›¾è°±å¯è§†åŒ–...")
        
        if 'graph' not in self.graph_data:
            print("âŒ æ— æ³•åŠ è½½å›¾æ•°æ®è¿›è¡Œå¯è§†åŒ–")
            return
        
        G = self.graph_data['graph']
        
        # åˆ›å»ºå­å›¾æ˜¾ç¤º
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        fig.suptitle('æç¤ºè¯å†™ä½œå›¾ä¹¦ - çŸ¥è¯†å›¾è°±å¯è§†åŒ–', fontsize=16, fontweight='bold')
        
        # 1. æ•´ä½“ç½‘ç»œå›¾
        ax1 = axes[0, 0]
        pos = nx.spring_layout(G, k=1, iterations=50)
        
        # æ ¹æ®åº¦æ•°è°ƒæ•´èŠ‚ç‚¹å¤§å°
        degrees = dict(G.degree())
        node_sizes = [degrees[node] * 20 + 50 for node in G.nodes()]
        
        nx.draw(G, pos, ax=ax1, node_size=node_sizes, node_color='lightblue', 
                edge_color='gray', alpha=0.7, with_labels=False)
        ax1.set_title('æ•´ä½“çŸ¥è¯†ç½‘ç»œ', fontsize=14)
        
        # 2. åº¦æ•°åˆ†å¸ƒ
        ax2 = axes[0, 1]
        degree_sequence = sorted([d for n, d in G.degree()], reverse=True)
        ax2.hist(degree_sequence, bins=20, alpha=0.7, color='skyblue')
        ax2.set_title('èŠ‚ç‚¹åº¦æ•°åˆ†å¸ƒ', fontsize=14)
        ax2.set_xlabel('åº¦æ•°')
        ax2.set_ylabel('é¢‘æ¬¡')
        
        # 3. ä¸­å¿ƒæ€§åˆ†æ
        ax3 = axes[1, 0]
        centrality = nx.betweenness_centrality(G)
        central_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]
        
        nodes, values = zip(*central_nodes) if central_nodes else ([], [])
        ax3.barh(range(len(nodes)), values)
        ax3.set_yticks(range(len(nodes)))
        ax3.set_yticklabels([str(node)[:20] for node in nodes])
        ax3.set_title('ä¸­é—´ä¸­å¿ƒæ€§æ’åå‰10çš„èŠ‚ç‚¹', fontsize=14)
        ax3.set_xlabel('ä¸­é—´ä¸­å¿ƒæ€§å€¼')
        
        # 4. ç¤¾åŒºæ£€æµ‹
        ax4 = axes[1, 1]
        try:
            communities = nx.community.greedy_modularity_communities(G.to_undirected())
            community_sizes = [len(c) for c in communities]
            ax4.pie(community_sizes, labels=[f'ç¤¾åŒº{i+1}' for i in range(len(communities))], 
                   autopct='%1.1f%%', startangle=90)
            ax4.set_title(f'ç¤¾åŒºç»“æ„ (å…±{len(communities)}ä¸ªç¤¾åŒº)', fontsize=14)
        except:
            ax4.text(0.5, 0.5, 'æ— æ³•è¿›è¡Œç¤¾åŒºæ£€æµ‹', ha='center', va='center', transform=ax4.transAxes)
            ax4.set_title('ç¤¾åŒºç»“æ„', fontsize=14)
        
        plt.tight_layout()
        plt.savefig('knowledge_graph_visualization.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("âœ… çŸ¥è¯†å›¾è°±å¯è§†åŒ–å·²ä¿å­˜ä¸º knowledge_graph_visualization.png")
    
    def create_entity_analysis(self):
        """åˆ›å»ºå®ä½“åˆ†æå›¾è¡¨"""
        print("\nğŸ“ˆ ç”Ÿæˆå®ä½“åˆ†æå›¾è¡¨...")
        
        if not self.entities:
            print("âŒ æ— æ³•åŠ è½½å®ä½“æ•°æ®è¿›è¡Œåˆ†æ")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('å®ä½“åˆ†æ - æç¤ºè¯å†™ä½œå›¾ä¹¦', fontsize=16, fontweight='bold')
        
        # 1. å®ä½“ç±»å‹åˆ†å¸ƒ
        ax1 = axes[0, 0]
        entity_types = Counter()
        for entity in self.entities:
            if 'type' in entity:
                entity_types[entity['type']] += 1
        
        if entity_types:
            top_types = dict(entity_types.most_common(10))
            ax1.bar(range(len(top_types)), list(top_types.values()), color='lightcoral')
            ax1.set_xticks(range(len(top_types)))
            ax1.set_xticklabels(list(top_types.keys()), rotation=45, ha='right')
            ax1.set_title('å®ä½“ç±»å‹åˆ†å¸ƒ', fontsize=14)
            ax1.set_ylabel('æ•°é‡')
        
        # 2. å®ä½“æè¿°é•¿åº¦åˆ†å¸ƒ
        ax2 = axes[0, 1]
        desc_lengths = []
        for entity in self.entities:
            if 'description' in entity and entity['description']:
                desc_lengths.append(len(entity['description']))
        
        if desc_lengths:
            ax2.hist(desc_lengths, bins=20, alpha=0.7, color='lightgreen')
            ax2.set_title('å®ä½“æè¿°é•¿åº¦åˆ†å¸ƒ', fontsize=14)
            ax2.set_xlabel('æè¿°é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰')
            ax2.set_ylabel('é¢‘æ¬¡')
        
        # 3. å…³é”®è¯äº‘ï¼ˆæ¨¡æ‹Ÿï¼‰
        ax3 = axes[1, 0]
        keywords = []
        for entity in self.entities:
            if 'name' in entity:
                keywords.extend(entity['name'].split())
        
        if keywords:
            keyword_counts = Counter(keywords)
            top_keywords = dict(keyword_counts.most_common(15))
            ax3.bar(range(len(top_keywords)), list(top_keywords.values()), color='lightskyblue')
            ax3.set_xticks(range(len(top_keywords)))
            ax3.set_xticklabels(list(top_keywords.keys()), rotation=45, ha='right')
            ax3.set_title('é«˜é¢‘å…³é”®è¯', fontsize=14)
            ax3.set_ylabel('å‡ºç°æ¬¡æ•°')
        
        # 4. ç»Ÿè®¡æ‘˜è¦
        ax4 = axes[1, 1]
        stats_text = f"""
        ç»Ÿè®¡æ‘˜è¦:
        
        â€¢ æ€»å®ä½“æ•°: {len(self.entities)}
        â€¢ å¹³å‡æè¿°é•¿åº¦: {sum(desc_lengths)/len(desc_lengths):.1f} å­—ç¬¦
        â€¢ å®ä½“ç±»å‹æ•°: {len(entity_types)}
        â€¢ æœ€å¸¸è§ç±»å‹: {entity_types.most_common(1)[0][0] if entity_types else 'N/A'}
        â€¢ æœ€é«˜é¢‘è¯: {keyword_counts.most_common(1)[0][0] if keywords else 'N/A'}
        """
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=12, 
                verticalalignment='top', bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        ax4.set_title('ç»Ÿè®¡æ‘˜è¦', fontsize=14)
        
        plt.tight_layout()
        plt.savefig('entity_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("âœ… å®ä½“åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º entity_analysis.png")

class PromptWritingQueryGenerator:
    """æç¤ºè¯å†™ä½œæ·±åº¦æŸ¥è¯¢ç”Ÿæˆå™¨"""
    
    def __init__(self, ragtest_path):
        self.ragtest_path = Path(ragtest_path)
        self.questions = []
        
    def generate_deep_questions(self):
        """ç”Ÿæˆ10ä¸ªæ·±åº¦æŒ–æ˜æç¤ºè¯å†™ä½œçš„è‹±æ–‡é—®é¢˜"""
        print("\nğŸ¤” ç”Ÿæˆæç¤ºè¯å†™ä½œæ·±åº¦æŒ–æ˜é—®é¢˜...")
        
        # å®šä¹‰é—®é¢˜ç±»åˆ«å’Œå¯¹åº”çš„æŸ¥è¯¢æ–¹æ³•
        questions_data = [
            {
                "question": "What are the fundamental frameworks and methodologies for effective prompt engineering?",
                "method": "global",
                "category": "Framework",
                "focus": "Overall structure and systematic approaches to prompt design"
            },
            {
                "question": "How do different prompt writing techniques compare in terms of effectiveness and use cases?",
                "method": "global", 
                "category": "Comparison",
                "focus": "Comparative analysis of various prompting methods"
            },
            {
                "question": "What are the specific step-by-step processes for constructing high-quality prompts?",
                "method": "local",
                "category": "Process",
                "focus": "Detailed procedural knowledge for prompt construction"
            },
            {
                "question": "Can you provide concrete examples of successful prompt templates for different AI tasks?",
                "method": "local",
                "category": "Examples",
                "focus": "Specific prompt examples and templates"
            },
            {
                "question": "What are the common pitfalls and mistakes to avoid when writing prompts?",
                "method": "local",
                "category": "Best Practices",
                "focus": "Error prevention and optimization techniques"
            },
            {
                "question": "How has prompt engineering evolved and what are the emerging trends in this field?",
                "method": "drift",
                "category": "Evolution",
                "focus": "Historical development and future directions"
            },
            {
                "question": "What role does context length and structure play in prompt effectiveness?",
                "method": "local",
                "category": "Technical",
                "focus": "Technical aspects of prompt design"
            },
            {
                "question": "How do domain-specific prompting strategies differ across various industries and applications?",
                "method": "global",
                "category": "Domain-Specific",
                "focus": "Industry-specific applications and adaptations"
            },
            {
                "question": "What are the psychological and cognitive principles behind effective prompt design?",
                "method": "drift",
                "category": "Psychology",
                "focus": "Human-AI interaction psychology"
            },
            {
                "question": "How can prompt writers measure and evaluate the quality and performance of their prompts?",
                "method": "local",
                "category": "Evaluation",
                "focus": "Metrics and assessment methods"
            }
        ]
        
        self.questions = questions_data
        return questions_data
    
    def execute_queries(self, max_queries=5):
        """æ‰§è¡ŒGraphRAGæŸ¥è¯¢ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼Œä»…æ˜¾ç¤ºå‰5ä¸ªï¼‰"""
        print(f"\nğŸ” æ‰§è¡ŒGraphRAGæŸ¥è¯¢ï¼ˆæ¼”ç¤ºå‰{max_queries}ä¸ªé—®é¢˜ï¼‰...")
        
        results = []
        
        for i, q_data in enumerate(self.questions[:max_queries]):
            print(f"\né—®é¢˜ {i+1}: {q_data['question']}")
            print(f"æŸ¥è¯¢æ–¹æ³•: {q_data['method']}")
            print(f"ç±»åˆ«: {q_data['category']}")
            
            # æ„å»ºæŸ¥è¯¢å‘½ä»¤
            cmd = f'conda activate graphrag-0.50 && graphrag query --root {self.ragtest_path} --method {q_data["method"]} --query "{q_data["question"]}"'
            
            print(f"æ‰§è¡Œå‘½ä»¤: {cmd}")
            print("=" * 80)
            
            # å®é™…é¡¹ç›®ä¸­å¯ä»¥æ‰§è¡ŒæŸ¥è¯¢
            # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è·³è¿‡å®é™…æ‰§è¡Œ
            results.append({
                "question": q_data["question"],
                "method": q_data["method"],
                "category": q_data["category"],
                "command": cmd,
                "status": "å‡†å¤‡æ‰§è¡Œ"
            })
        
        return results
    
    def save_questions_to_file(self):
        """ä¿å­˜é—®é¢˜åˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"prompt_writing_deep_questions_{timestamp}.md"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("# æç¤ºè¯å†™ä½œæ·±åº¦æŒ–æ˜é—®é¢˜é›†\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("## é—®é¢˜åˆ—è¡¨\n\n")
            
            for i, q_data in enumerate(self.questions, 1):
                f.write(f"### é—®é¢˜ {i}\n\n")
                f.write(f"**é—®é¢˜**: {q_data['question']}\n\n")
                f.write(f"**æŸ¥è¯¢æ–¹æ³•**: `{q_data['method']}`\n\n")
                f.write(f"**ç±»åˆ«**: {q_data['category']}\n\n")
                f.write(f"**å…³æ³¨ç‚¹**: {q_data['focus']}\n\n")
                f.write(f"**æ‰§è¡Œå‘½ä»¤**:\n```bash\n")
                f.write(f"conda activate graphrag-0.50\n")
                f.write(f"graphrag query --root ./ragtest --method {q_data['method']} --query \"{q_data['question']}\"\n")
                f.write("```\n\n")
                f.write("---\n\n")
            
            # æ·»åŠ æŸ¥è¯¢æ–¹æ³•è¯´æ˜
            f.write("## æŸ¥è¯¢æ–¹æ³•è¯´æ˜\n\n")
            f.write("- **local**: é€‚ç”¨äºéœ€è¦å…·ä½“ç»†èŠ‚ã€ç‰¹å®šç¤ºä¾‹æˆ–å±€éƒ¨ä¿¡æ¯çš„é—®é¢˜\n")
            f.write("- **global**: é€‚ç”¨äºéœ€è¦æ•´ä½“æ¦‚è¿°ã€æ¯”è¾ƒåˆ†ææˆ–å®è§‚è§†è§’çš„é—®é¢˜\n")
            f.write("- **drift**: é€‚ç”¨äºéœ€è¦æ¢ç´¢å…³è”æ€§ã€å‘ç°æ–°æ¨¡å¼æˆ–è¶‹åŠ¿çš„é—®é¢˜\n\n")
            
        print(f"âœ… é—®é¢˜é›†å·²ä¿å­˜åˆ° {filename}")
        return filename

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GraphRAG æç¤ºè¯å†™ä½œå›¾ä¹¦å¯è§†åŒ–ä¸æ·±åº¦æŸ¥è¯¢ç³»ç»Ÿ")
    print("=" * 60)
    
    # è®¾ç½®è·¯å¾„
    artifacts_path = "C:/Users/13694/ragtest/output/20250602-151653/artifacts"
    ragtest_path = "C:/Users/13694/ragtest"
    
    # åˆå§‹åŒ–å¯è§†åŒ–å™¨
    visualizer = GraphRAGVisualizer(artifacts_path)
    
    # 1. åŠ è½½å’Œåˆ†ææ•°æ®
    print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šåŠ è½½å’Œåˆ†æå›¾ä¹¦æ•°æ®")
    visualizer.load_graph_data()
    entity_types, descriptions = visualizer.analyze_book_structure()
    
    # 2. ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ¨ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    visualizer.visualize_knowledge_graph()
    visualizer.create_entity_analysis()
    
    # 3. ç”Ÿæˆæ·±åº¦æŸ¥è¯¢é—®é¢˜
    print("\nğŸ¤” ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæç¤ºè¯å†™ä½œæ·±åº¦æŸ¥è¯¢é—®é¢˜")
    query_generator = PromptWritingQueryGenerator(ragtest_path)
    questions = query_generator.generate_deep_questions()
    
    # 4. æ˜¾ç¤ºé—®é¢˜æ¦‚è§ˆ
    print("\nğŸ“‹ ç”Ÿæˆçš„10ä¸ªæ·±åº¦æŒ–æ˜é—®é¢˜:")
    for i, q in enumerate(questions, 1):
        print(f"\n{i}. [{q['method'].upper()}] {q['category']}")
        print(f"   {q['question']}")
    
    # 5. ä¿å­˜é—®é¢˜åˆ°æ–‡ä»¶
    query_generator.save_questions_to_file()
    
    # 6. æ¼”ç¤ºæŸ¥è¯¢æ‰§è¡Œ
    print("\nğŸ” ç¬¬å››æ­¥ï¼šæ¼”ç¤ºæŸ¥è¯¢æ‰§è¡Œ")
    query_generator.execute_queries(max_queries=3)
    
    print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - knowledge_graph_visualization.png: çŸ¥è¯†å›¾è°±å¯è§†åŒ–")
    print("  - entity_analysis.png: å®ä½“åˆ†æå›¾è¡¨")
    print("  - prompt_writing_deep_questions_*.md: æ·±åº¦æŸ¥è¯¢é—®é¢˜é›†")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("  1. æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨äº†è§£å›¾ä¹¦ç»“æ„")
    print("  2. ä½¿ç”¨é—®é¢˜é›†ä¸­çš„å‘½ä»¤æ‰§è¡ŒGraphRAGæŸ¥è¯¢")
    print("  3. æ ¹æ®æŸ¥è¯¢ç»“æœè°ƒæ•´å’Œä¼˜åŒ–é—®é¢˜")

if __name__ == "__main__":
    main() 